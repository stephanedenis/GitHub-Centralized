#!/usr/bin/env python3
"""
Dashboard S√©mantique Unifi√© - Version Text
==========================================

Dashboard textuel des analyses s√©mantiques sans d√©pendances matplotlib.

Date: 2025-10-02
Auteur: Syst√®me Autonome PaniniFS
Version: 1.0.0
"""

import json
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Any
from collections import Counter, defaultdict


class SemanticTextDashboard:
    """Dashboard s√©mantique textuel"""
    
    def __init__(self):
        self.orchestration_data = None
        self.pattern_data = None
        self.coherence_data = None
        self.translator_data = None
        
    def load_latest_orchestration(self) -> bool:
        """Charge le dernier rapport d'orchestration"""
        orchestration_files = list(Path('.').glob('*semantic_orchestration*.json'))
        
        if not orchestration_files:
            print("‚ùå Aucun rapport d'orchestration trouv√©")
            return False
        
        latest = max(orchestration_files, key=lambda f: f.stat().st_mtime)
        
        try:
            with open(latest, 'r', encoding='utf-8') as f:
                self.orchestration_data = json.load(f)
            print(f"‚úÖ Orchestration charg√©e: {latest.name}")
            return True
        except Exception as e:
            print(f"‚ùå Erreur chargement orchestration: {e}")
            return False
    
    def load_supporting_data(self):
        """Charge donn√©es de support des autres analyseurs"""
        
        # Patterns s√©mantiques
        pattern_files = list(Path('.').glob('*semantic_patterns_analysis*.json'))
        if pattern_files:
            latest_pattern = max(pattern_files, key=lambda f: f.stat().st_mtime)
            try:
                with open(latest_pattern, 'r', encoding='utf-8') as f:
                    self.pattern_data = json.load(f)
                print(f"‚úÖ Patterns charg√©s: {latest_pattern.name}")
            except:
                pass
        
        # Coh√©rence s√©mantique
        coherence_files = list(Path('.').glob('*semantic_coherence_analysis*.json'))
        if coherence_files:
            latest_coherence = max(coherence_files, key=lambda f: f.stat().st_mtime)
            try:
                with open(latest_coherence, 'r', encoding='utf-8') as f:
                    self.coherence_data = json.load(f)
                print(f"‚úÖ Coh√©rence charg√©e: {latest_coherence.name}")
            except:
                pass
        
        # Biais traducteurs
        translator_files = list(Path('.').glob('*translator_bias*.json'))
        if translator_files:
            latest_translator = max(translator_files, key=lambda f: f.stat().st_mtime)
            try:
                with open(latest_translator, 'r', encoding='utf-8') as f:
                    self.translator_data = json.load(f)
                print(f"‚úÖ Biais traducteurs charg√©s: {latest_translator.name}")
            except:
                pass
    
    def display_orchestration_overview(self):
        """Affiche vue d'ensemble orchestration"""
        
        print("\n" + "="*60)
        print("üìä VUE D'ENSEMBLE ORCHESTRATION")
        print("="*60)
        
        if not self.orchestration_data:
            print("‚ùå Aucune donn√©e d'orchestration disponible")
            return
        
        summary = self.orchestration_data['summary']
        
        print(f"üîÑ Analyseurs:")
        print(f"   ‚Ä¢ Total: {summary['total_analyzers']}")
        print(f"   ‚Ä¢ Succ√®s: {summary['successful_analyzers']} ({summary['successful_analyzers']/summary['total_analyzers']*100:.1f}%)")
        print(f"   ‚Ä¢ √âchecs: {summary['failed_analyzers']} ({summary['failed_analyzers']/summary['total_analyzers']*100:.1f}%)")
        
        print(f"\n‚è±Ô∏è  Performance:")
        print(f"   ‚Ä¢ Temps total: {summary['total_execution_time']:.2f}s")
        print(f"   ‚Ä¢ Fichiers g√©n√©r√©s: {len(summary['output_files'])}")
        
        print(f"\nüîç Validation crois√©e:")
        print(f"   ‚Ä¢ Score: {summary['cross_validation_score']:.3f}")
        
        # D√©tails par analyseur
        print(f"\nüìã D√âTAILS ANALYSEURS:")
        print("-"*40)
        
        analyzer_results = self.orchestration_data.get('analyzer_results', [])
        for result in analyzer_results:
            status = "‚úÖ" if result['success'] else "‚ùå"
            print(f"{status} {result['analyzer_name']:<20} {result['execution_time']:.2f}s")
            if not result['success'] and result.get('error_message'):
                print(f"    Erreur: {result['error_message'][:80]}...")
        
        # Insights
        insights = summary.get('unified_insights', [])
        if insights:
            print(f"\nüí° INSIGHTS UNIFI√âS:")
            for insight in insights:
                print(f"   ‚Ä¢ {insight}")
    
    def display_patterns_analysis(self):
        """Affiche analyse patterns"""
        
        print("\n" + "="*60)
        print("üß† ANALYSE PATTERNS S√âMANTIQUES")
        print("="*60)
        
        if not self.pattern_data:
            print("‚ùå Aucune donn√©e de patterns disponible")
            return
        
        patterns = self.pattern_data.get('patterns', [])
        invariants = self.pattern_data.get('universal_invariants', [])
        
        print(f"üìä STATISTIQUES G√âN√âRALES:")
        print(f"   ‚Ä¢ Patterns d√©tect√©s: {len(patterns)}")
        print(f"   ‚Ä¢ Invariants universels: {len(invariants)}")
        print(f"   ‚Ä¢ Analyseurs sources: {self.pattern_data['meta']['analyzers_count']}")
        
        # Patterns par type
        if patterns:
            pattern_types = [p['pattern_type'] for p in patterns]
            type_counts = Counter(pattern_types)
            
            print(f"\nüè∑Ô∏è  PATTERNS PAR TYPE:")
            for pattern_type, count in type_counts.items():
                print(f"   ‚Ä¢ {pattern_type.capitalize()}: {count}")
            
            # Patterns universaux
            universal_patterns = [p for p in patterns if p['pattern_type'] == 'universal']
            if universal_patterns:
                print(f"\nüåü PATTERNS UNIVERSAUX ({len(universal_patterns)}):")
                for pattern in universal_patterns:
                    print(f"   ‚Ä¢ {pattern['pattern_id']}")
                    print(f"     Force: {pattern['strength']:.3f} | Confiance: {pattern['confidence']:.3f}")
                    print(f"     Sources: {', '.join(pattern['sources'])}")
        
        # Invariants universels
        if invariants:
            print(f"\nüîó INVARIANTS UNIVERSELS ({len(invariants)}):")
            for invariant in invariants:
                print(f"   ‚Ä¢ {invariant['invariant_id']}")
                print(f"     Description: {invariant['description']}")
                print(f"     Score universalit√©: {invariant['universality_score']:.3f}")
                if invariant.get('mathematical_form'):
                    print(f"     Forme: {invariant['mathematical_form']}")
    
    def display_coherence_analysis(self):
        """Affiche analyse coh√©rence"""
        
        print("\n" + "="*60)
        print("üîç ANALYSE COH√âRENCE S√âMANTIQUE")
        print("="*60)
        
        if not self.coherence_data:
            print("‚ùå Aucune donn√©e de coh√©rence disponible")
            return
        
        stats = self.coherence_data.get('statistics', {})
        violations = self.coherence_data.get('coherence_violations', [])
        invariants = self.coherence_data.get('semantic_invariants', [])
        domains = self.coherence_data.get('domains', {})
        
        print(f"üìä STATISTIQUES G√âN√âRALES:")
        print(f"   ‚Ä¢ Domaines analys√©s: {len(domains)}")
        print(f"   ‚Ä¢ Violations d√©tect√©es: {len(violations)}")
        print(f"   ‚Ä¢ Invariants d√©tect√©s: {len(invariants)}")
        print(f"   ‚Ä¢ Score coh√©rence: {stats.get('coherence_score', 0):.3f}")
        
        # Domaines
        print(f"\nüè∑Ô∏è  DOMAINES ANALYS√âS:")
        for domain_name, domain_info in domains.items():
            print(f"   ‚Ä¢ {domain_name}: {domain_info['file']}")
        
        # Violations par type
        if violations:
            violation_types = [v['violation_type'] for v in violations]
            type_counts = Counter(violation_types)
            
            print(f"\n‚ö†Ô∏è  VIOLATIONS PAR TYPE:")
            for vtype, count in type_counts.items():
                print(f"   ‚Ä¢ {vtype}: {count}")
            
            # Violations par s√©v√©rit√©
            severities = [v['severity'] for v in violations]
            severity_counts = Counter(severities)
            
            print(f"\nüö® VIOLATIONS PAR S√âV√âRIT√â:")
            severity_order = ['critical', 'high', 'medium', 'low']
            for severity in severity_order:
                if severity in severity_counts:
                    count = severity_counts[severity]
                    emoji = {'critical': 'üî¥', 'high': 'üü†', 'medium': 'üü°', 'low': 'üü¢'}[severity]
                    print(f"   {emoji} {severity.capitalize()}: {count}")
            
            # Top violations
            print(f"\nüîç TOP VIOLATIONS:")
            sorted_violations = sorted(violations, key=lambda v: v['confidence'], reverse=True)[:5]
            for i, violation in enumerate(sorted_violations, 1):
                print(f"   {i}. {violation['violation_type']} ({violation['severity']})")
                print(f"      Domaines: {', '.join(violation['domains'])}")
                print(f"      Description: {violation['description']}")
                print(f"      Confiance: {violation['confidence']:.3f}")
    
    def display_translator_analysis(self):
        """Affiche analyse traducteurs"""
        
        print("\n" + "="*60)
        print("üë• ANALYSE BIAIS TRADUCTEURS")
        print("="*60)
        
        if not self.translator_data:
            print("‚ùå Aucune donn√©e de traducteurs disponible")
            return
        
        print(f"üìä STATISTIQUES G√âN√âRALES:")
        print(f"   ‚Ä¢ Traducteurs analys√©s: {self.translator_data.get('traducteurs_analyses', 0)}")
        
        # Patterns culturels
        cultural_patterns = self.translator_data.get('patterns_culturels', {})
        if cultural_patterns:
            print(f"\nüåç PATTERNS CULTURELS ({len(cultural_patterns)}):")
            for pattern_name, translators in cultural_patterns.items():
                print(f"   ‚Ä¢ {pattern_name}: {len(translators)} traducteur(s)")
                if len(translators) <= 3:  # Afficher noms si peu
                    print(f"     Traducteurs: {', '.join(translators)}")
        
        # Patterns temporels
        temporal_patterns = self.translator_data.get('patterns_temporels', {})
        if temporal_patterns:
            print(f"\n‚è∞ PATTERNS TEMPORELS ({len(temporal_patterns)}):")
            for pattern_name, translators in temporal_patterns.items():
                print(f"   ‚Ä¢ {pattern_name}: {len(translators)} traducteur(s)")
        
        # Signatures stylistiques
        style_signatures = self.translator_data.get('signatures_stylistiques', {})
        if style_signatures:
            print(f"\n‚úçÔ∏è  SIGNATURES STYLISTIQUES ({len(style_signatures)}):")
            for signature_name, translators in style_signatures.items():
                print(f"   ‚Ä¢ {signature_name}: {len(translators)} traducteur(s)")
        
        # Universaux vs contextuels
        universaux = self.translator_data.get('universaux', {})
        contextuels = self.translator_data.get('contextuels', {})
        
        if universaux or contextuels:
            print(f"\nüåê UNIVERSAUX vs CONTEXTUELS:")
            print(f"   ‚Ä¢ Patterns universaux: {len(universaux)}")
            print(f"   ‚Ä¢ Patterns contextuels: {len(contextuels)}")
            
            if universaux:
                print(f"\nüîó CANDIDATS UNIVERSAUX:")
                for pattern_name, info in universaux.items():
                    print(f"   ‚Ä¢ {pattern_name}: validation sur {info.get('count', 0)} traducteur(s)")
    
    def generate_executive_summary(self) -> Dict:
        """G√©n√®re r√©sum√© ex√©cutif"""
        
        timestamp = datetime.now(timezone.utc).isoformat()
        
        # M√©triques cl√©s
        total_patterns = 0
        universal_patterns = 0
        total_violations = 0
        critical_violations = 0
        coherence_score = 0.0
        success_rate = 0.0
        
        if self.pattern_data:
            patterns = self.pattern_data.get('patterns', [])
            total_patterns = len(patterns)
            universal_patterns = len([p for p in patterns if p.get('pattern_type') == 'universal'])
        
        if self.coherence_data:
            violations = self.coherence_data.get('coherence_violations', [])
            total_violations = len(violations)
            critical_violations = len([v for v in violations if v.get('severity') == 'critical'])
            coherence_score = self.coherence_data.get('statistics', {}).get('coherence_score', 0.0)
        
        if self.orchestration_data:
            summary = self.orchestration_data['summary']
            success_rate = summary['successful_analyzers'] / summary['total_analyzers']
        
        # Score global
        global_score = self._calculate_global_score()
        
        # Recommandations
        recommendations = self._generate_recommendations()
        
        # Insights cl√©s
        key_insights = self._extract_key_insights()
        
        summary = {
            "meta": {
                "timestamp": timestamp,
                "dashboard_version": "1.0.0 (text)",
                "analysis_scope": "semantic_ecosystem"
            },
            "executive_summary": {
                "global_semantic_score": global_score,
                "analyzer_success_rate": success_rate,
                "patterns_detected": total_patterns,
                "universal_patterns": universal_patterns,
                "coherence_violations": total_violations,
                "critical_violations": critical_violations,
                "coherence_score": coherence_score
            },
            "key_insights": key_insights,
            "recommendations": recommendations,
            "data_completeness": {
                "orchestration": self.orchestration_data is not None,
                "patterns": self.pattern_data is not None,
                "coherence": self.coherence_data is not None,
                "translators": self.translator_data is not None
            }
        }
        
        return summary
    
    def _calculate_global_score(self) -> float:
        """Calcule score global"""
        
        scores = []
        
        # Score orchestration
        if self.orchestration_data:
            summary = self.orchestration_data['summary']
            success_rate = summary['successful_analyzers'] / summary['total_analyzers']
            scores.append(success_rate)
        
        # Score patterns (ratio universaux)
        if self.pattern_data:
            patterns = self.pattern_data.get('patterns', [])
            if patterns:
                universal_count = len([p for p in patterns if p.get('pattern_type') == 'universal'])
                universal_ratio = universal_count / len(patterns)
                scores.append(universal_ratio)
        
        # Score coh√©rence
        if self.coherence_data:
            coherence_score = self.coherence_data.get('statistics', {}).get('coherence_score', 0.0)
            scores.append(coherence_score)
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _generate_recommendations(self) -> List[str]:
        """G√©n√®re recommandations"""
        
        recommendations = []
        
        # Recommandations orchestration
        if self.orchestration_data:
            failed_analyzers = [r for r in self.orchestration_data.get('analyzer_results', []) if not r['success']]
            if failed_analyzers:
                recommendations.append(f"R√©parer {len(failed_analyzers)} analyseurs d√©faillants")
        
        # Recommandations coh√©rence
        if self.coherence_data:
            violations = self.coherence_data.get('coherence_violations', [])
            critical = [v for v in violations if v.get('severity') == 'critical']
            if critical:
                recommendations.append(f"R√©soudre d'urgence {len(critical)} violations critiques")
        
        # Recommandations patterns
        if self.pattern_data:
            patterns = self.pattern_data.get('patterns', [])
            universal_patterns = [p for p in patterns if p.get('pattern_type') == 'universal']
            if len(universal_patterns) == 0:
                recommendations.append("Aucun pattern universel d√©tect√© - r√©viser m√©thodologie")
        
        global_score = self._calculate_global_score()
        if global_score < 0.5:
            recommendations.append("Score global faible - audit complet recommand√©")
        
        return recommendations
    
    def _extract_key_insights(self) -> List[str]:
        """Extrait insights cl√©s"""
        
        insights = []
        
        # Insight patterns
        if self.pattern_data:
            patterns = self.pattern_data.get('patterns', [])
            universal_patterns = [p for p in patterns if p.get('pattern_type') == 'universal']
            if universal_patterns:
                insights.append(f"Base solide: {len(universal_patterns)} patterns universaux valid√©s")
        
        # Insight coh√©rence
        if self.coherence_data:
            domains_count = len(self.coherence_data.get('domains', {}))
            insights.append(f"Analyse coh√©rence cross-domain sur {domains_count} domaines")
        
        # Insight performance
        if self.orchestration_data:
            exec_time = self.orchestration_data['summary']['total_execution_time']
            if exec_time < 1.0:
                insights.append("Performance excellente: analyse compl√®te sous 1 seconde")
        
        # Insight traducteurs
        if self.translator_data:
            cultural_patterns = self.translator_data.get('patterns_culturels', {})
            universaux = [p for p, translators in cultural_patterns.items() 
                         if len(translators) >= self.translator_data.get('traducteurs_analyses', 1)]
            if universaux:
                insights.append(f"Biais culturels universels confirm√©s: {len(universaux)} patterns")
        
        return insights
    
    def run_full_dashboard(self) -> bool:
        """G√©n√®re dashboard complet"""
        
        print("\nüìä DASHBOARD S√âMANTIQUE TEXTUEL - G√âN√âRATION COMPL√àTE")
        print("=" * 70)
        
        # 1. Chargement donn√©es
        if not self.load_latest_orchestration():
            return False
        
        self.load_supporting_data()
        
        # 2. Affichage sections
        self.display_orchestration_overview()
        self.display_patterns_analysis()
        self.display_coherence_analysis()
        self.display_translator_analysis()
        
        # 3. R√©sum√© ex√©cutif
        print("\n" + "="*70)
        print("üìã R√âSUM√â EX√âCUTIF")
        print("="*70)
        
        summary = self.generate_executive_summary()
        
        exec_summary = summary['executive_summary']
        print(f"üéØ Score Global S√©mantique: {exec_summary['global_semantic_score']:.3f}")
        print(f"‚úÖ Taux succ√®s analyseurs: {exec_summary['analyzer_success_rate']:.1%}")
        print(f"üß† Patterns d√©tect√©s: {exec_summary['patterns_detected']} (dont {exec_summary['universal_patterns']} universaux)")
        print(f"üîç Score coh√©rence: {exec_summary['coherence_score']:.3f}")
        print(f"‚ö†Ô∏è  Violations: {exec_summary['coherence_violations']} (dont {exec_summary['critical_violations']} critiques)")
        
        # Insights cl√©s
        print(f"\nüí° INSIGHTS CL√âS:")
        for insight in summary['key_insights']:
            print(f"   ‚Ä¢ {insight}")
        
        # Recommandations
        print(f"\nüìã RECOMMANDATIONS:")
        for recommendation in summary['recommendations']:
            print(f"   ‚Ä¢ {recommendation}")
        
        # 4. Sauvegarde rapport
        timestamp = datetime.now(timezone.utc).isoformat()
        summary_file = f"semantic_dashboard_summary_{timestamp.replace(':', '-').replace('.', '-')[:19]}Z.json"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ Rapport sauvegard√©: {summary_file}")
        
        return True


def main():
    """Point d'entr√©e principal"""
    
    dashboard = SemanticTextDashboard()
    
    try:
        success = dashboard.run_full_dashboard()
        
        if success:
            print(f"\n‚úÖ DASHBOARD S√âMANTIQUE TERMIN√â AVEC SUCC√àS")
            print("="*50)
            
            global_score = dashboard._calculate_global_score()
            
            if global_score > 0.8:
                print("üåü Excellent - √âcosyst√®me s√©mantique tr√®s robuste")
            elif global_score > 0.6:
                print("‚úÖ Bon - √âcosyst√®me s√©mantique fonctionnel")
            elif global_score > 0.4:
                print("‚ö†Ô∏è  Moyen - Am√©liorations n√©cessaires")
            else:
                print("üö® Faible - R√©vision majeure requise")
        
        return success
        
    except Exception as e:
        print(f"‚ùå ERREUR DASHBOARD: {e}")
        return False


if __name__ == "__main__":
    main()